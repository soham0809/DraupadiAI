{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(audio_path, save_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "# Directories\n",
    "data_dir = \"./dataset\"  # Your root folder\n",
    "scream_dir = os.path.join(data_dir, \"scream\")\n",
    "non_scream_dir = os.path.join(data_dir, \"non_scream\")\n",
    "spectrogram_dir = \"./spectrograms\"\n",
    "\n",
    "# Create spectrogram folders\n",
    "os.makedirs(os.path.join(spectrogram_dir, \"scream\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(spectrogram_dir, \"non_scream\"), exist_ok=True)\n",
    "\n",
    "# Generate and save spectrograms\n",
    "for folder, label in [(scream_dir, \"scream\"), (non_scream_dir, \"non_scream\")]:\n",
    "    for file_name in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        save_path = os.path.join(spectrogram_dir, label, file_name.replace(\".wav\", \".png\"))\n",
    "        create_spectrogram(file_path, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, spectrogram_dir, transform=None):\n",
    "        self.spectrogram_dir = spectrogram_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label, folder in enumerate([\"non_scream\", \"scream\"]):\n",
    "            folder_path = os.path.join(spectrogram_dir, folder)\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                self.data.append(os.path.join(folder_path, file_name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create Dataset\n",
    "dataset = SpectrogramDataset(spectrogram_dir, transform=transform)\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # 3 input channels (RGB), 32 filters\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 64 filters\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Pooling layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Placeholder for the flattened size\n",
    "        self.fc1 = None\n",
    "        self.fc2 = nn.Linear(128, 2)  # 128 hidden units, 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Dynamically initialize the fc1 layer based on input size\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 128).to(x.device)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.9222, Accuracy: 92.85%\n",
      "Epoch [2/25], Loss: 0.0575, Accuracy: 98.04%\n",
      "Epoch [3/25], Loss: 0.0346, Accuracy: 98.84%\n",
      "Epoch [4/25], Loss: 0.0229, Accuracy: 99.32%\n",
      "Epoch [5/25], Loss: 0.0088, Accuracy: 99.56%\n",
      "Epoch [6/25], Loss: 0.0087, Accuracy: 99.72%\n",
      "Epoch [7/25], Loss: 0.0161, Accuracy: 99.44%\n",
      "Epoch [8/25], Loss: 0.0078, Accuracy: 99.68%\n",
      "Epoch [9/25], Loss: 0.0057, Accuracy: 99.80%\n",
      "Epoch [10/25], Loss: 0.0043, Accuracy: 99.84%\n",
      "Epoch [11/25], Loss: 0.0055, Accuracy: 99.84%\n",
      "Epoch [12/25], Loss: 0.0578, Accuracy: 98.20%\n",
      "Epoch [13/25], Loss: 0.0468, Accuracy: 98.28%\n",
      "Epoch [14/25], Loss: 0.0172, Accuracy: 99.28%\n",
      "Epoch [15/25], Loss: 0.0201, Accuracy: 99.44%\n",
      "Epoch [16/25], Loss: 0.0037, Accuracy: 99.96%\n",
      "Epoch [17/25], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [18/25], Loss: 0.0004, Accuracy: 100.00%\n",
      "Epoch [19/25], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [20/25], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [21/25], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [22/25], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [23/25], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [24/25], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [25/25], Loss: 0.0001, Accuracy: 100.00%\n",
      "Model saved to scream_detection_model.pth\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")\n",
    "\n",
    "MODEL_PATH = \"scream_detection_model.pth\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model saved to {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished.\n",
      "Scream\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def predict_live_audio(model, device):\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100\n",
    "    RECORD_SECONDS = 10\n",
    "    WAVE_OUTPUT_FILENAME = \"live_audio.wav\"\n",
    "\n",
    "    # Record audio\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"Recording finished.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    # Save audio file\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, \"wb\")\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    # Generate spectrogram\n",
    "    live_spec_path = \"live_spectrogram.png\"\n",
    "    create_spectrogram(WAVE_OUTPUT_FILENAME, live_spec_path)\n",
    "\n",
    "    # Predict\n",
    "    img = Image.open(live_spec_path).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return \"Scream\" if predicted.item() == 1 else \"Not Scream\"\n",
    "\n",
    "# Test\n",
    "print(predict_live_audio(model, device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
